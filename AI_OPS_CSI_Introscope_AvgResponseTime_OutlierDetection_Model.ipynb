{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "#from pyemma import msm # not available on Kaggle Kernel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://one.proxy.att.com:8080\"\n",
    "os.environ[\"https_proxy\"] = \"http://one.proxy.att.com:8080\"\n",
    "!pip install sklearn_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/home/jovyan/conf/master.csv')\n",
    "df = pd.read_csv('/home/jovyan/conf/master.csv',error_bad_lines=False)\n",
    "#df = pd.read_csv('CSI_Introscope_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['METRIC_ATTRIBUTE']\n",
    "for x in df.METRIC_ATTRIBUTE.unique():\n",
    "    df[x]=(df.METRIC_ATTRIBUTE==x).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Average Response Time (ms)'] = df.VALUE_COUNT.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Average Response Time (ms)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"aiops_csi_processed_data.csv\", index=False)\n",
    "df.to_csv(\"aiops_apm_processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill 0 for numbers and NNN for strings\n",
    "for colname, col in df.iteritems():\n",
    "    if(df[colname].dtype == np.object):\n",
    "        df[colname] = df[colname].fillna('empty')\n",
    "    else:\n",
    "        df[colname] = df[colname].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_time'] = pd.to_datetime(df['TS'])\n",
    "df = df.sort_values('date_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_time'].min(), df['date_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=['Average Response Time (ms)']\n",
    "continuous_columns=['MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']\n",
    "feature_columns = categorical_columns + continuous_columns\n",
    "output_columns=['Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['date_time', 'Average Response Time (ms)', 'MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['Average Response Time (ms)'] < 1218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Average Response Time (ms)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "df.plot(x='date_time', y='Average Response Time (ms)', figsize=(12,6))\n",
    "plt.xlabel('Date time')\n",
    "plt.ylabel('Average Response Time (ms)')\n",
    "plt.title('Time Series of average response time by date time of search');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['MIN_VALUE'] == 0, 'Average Response Time (ms)']\n",
    "b = df.loc[df['MIN_VALUE'] == 1, 'Average Response Time (ms)']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(a, bins = 50, alpha=0.5, label='very min value')\n",
    "plt.hist(b, bins = 50, alpha=0.5, label='min value')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Average Response Time')\n",
    "plt.ylabel('Count')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['MAX_VALUE'] == 0, 'Average Response Time (ms)']\n",
    "b = df.loc[df['MAX_VALUE'] == 1, 'Average Response Time (ms)']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(a, bins = 50, alpha=0.5, label='very max value')\n",
    "plt.hist(b, bins = 50, alpha=0.5, label='max value')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Average Response Time')\n",
    "plt.ylabel('Count')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['AGG_VALUE'] == 0, 'Average Response Time (ms)']\n",
    "b = df.loc[df['AGG_VALUE'] == 1, 'Average Response Time (ms)']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(a, bins = 50, alpha=0.5, label='large agg value')\n",
    "plt.hist(b, bins = 50, alpha=0.5, label='agg value')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Average Response Time')\n",
    "plt.ylabel('AGG_VALUE')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Point Analysis - Kmeans\n",
    "data = df[['Average Response Time (ms)','MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']]\n",
    "n_cluster = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Average Response Time (ms)','MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']]\n",
    "X = X.reset_index(drop=True)\n",
    "km = KMeans(n_clusters=7)\n",
    "km.fit(X)\n",
    "km.predict(X)\n",
    "labels = km.labels_\n",
    "#Plotting\n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "ax.scatter(X.iloc[:,0], X.iloc[:,1], X.iloc[:,2],\n",
    "          c=labels.astype(np.float), edgecolor=\"k\")\n",
    "ax.set_xlabel(\"Average Response Time (ms)\")\n",
    "ax.set_ylabel(\"VALUE_COUNT\")\n",
    "ax.set_zlabel(\"MIN_VALUE\")\n",
    "plt.title(\"K Means\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['Average Response Time (ms)','MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']]\n",
    "X = data.values\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "#Calculating Eigenvecors and eigenvalues of Covariance matrix\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "# Create a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "# Calculation of Explained Variance from the eigenvalues\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')\n",
    "plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take useful feature and standardize them\n",
    "data = df[['Average Response Time (ms)','MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']]\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "data = pd.DataFrame(X_std)\n",
    "# reduce to 2 important features\n",
    "pca = PCA(n_components=2)\n",
    "data = pca.fit_transform(data)\n",
    "# standardize these 2 new features\n",
    "scaler = StandardScaler()\n",
    "np_scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('date_time')\n",
    "df['date_time_int'] = df.date_time.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "df['cluster'] = kmeans[7].predict(data)\n",
    "df.index = data.index\n",
    "df['principal_feature1'] = data[0]\n",
    "df['principal_feature2'] = data[1]\n",
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return Series of distance between each point and its distance with the closest centroid\n",
    "def getDistanceByPoint(data, model):\n",
    "    distance = pd.Series()\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.set_value(i, np.linalg.norm(Xa-Xb))\n",
    "    return distance\n",
    "\n",
    "outliers_fraction = 0.01\n",
    "# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\n",
    "distance = getDistanceByPoint(data, kmeans[7])\n",
    "number_of_outliers = int(outliers_fraction*len(distance))\n",
    "threshold = distance.nlargest(number_of_outliers).min()\n",
    "# anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly) \n",
    "df['anomaly1'] = (distance >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "colors = {0:'blue', 1:'red'}\n",
    "ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"anomaly1\"].apply(lambda x: colors[x]))\n",
    "plt.xlabel('principal feature1')\n",
    "plt.ylabel('principal feature2')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.anomaly1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "a = df.loc[df['anomaly1'] == 1, ['date_time_int', 'Average Response Time (ms)']] #anomaly\n",
    "\n",
    "ax.plot(df['date_time_int'], df['Average Response Time (ms)'], color='blue', label='Normal')\n",
    "ax.scatter(a['date_time_int'],a['Average Response Time (ms)'], color='red', label='Anomaly')\n",
    "plt.xlabel('Date Time Integer')\n",
    "plt.ylabel('Average Response Time (ms)')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['anomaly1'] == 0, 'Average Response Time (ms)']\n",
    "b = df.loc[df['anomaly1'] == 1, 'Average Response Time (ms)']\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolation Forest\n",
    "data = df[['Average Response Time (ms)','MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']]\n",
    "#X = df[feature_columns]\n",
    "scaler = StandardScaler()\n",
    "np_scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "# train isolation forest\n",
    "model =  IsolationForest(contamination=0.02)\n",
    "model.fit(data)\n",
    "\n",
    "df['anomaly2'] = pd.Series(model.predict(data))\n",
    "# df['anomaly2'] = df['anomaly2'].map( {1: 0, -1: 1} )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "a = df.loc[df['anomaly2'] == -1, ['date_time_int', 'Average Response Time (ms)']] #anomaly\n",
    "\n",
    "ax.plot(df['date_time_int'], df['Average Response Time (ms)'], color='blue', label = 'Normal')\n",
    "ax.scatter(a['date_time_int'],a['Average Response Time (ms)'], color='red', label = 'Anomaly')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['anomaly2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of anomaly with Average Response\n",
    "a = df.loc[df['anomaly2'] == 1, 'Average Response Time (ms)']\n",
    "b = df.loc[df['anomaly2'] == -1, 'Average Response Time (ms)']\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Average Response Time (ms)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(model, \"ai_ops_csi_v3.pkl\")\n",
    "joblib.dump(model, \"aiops_apm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # Save to file in the current working directory\n",
    "pkl_filename = \"aiops_apm.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    saved_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.read_csv(\"aiops_csi_processed_response_data_test.csv\")\n",
    "#df_test = pd.read_csv(\"CSI_Introscope_data_Test.csv\")\n",
    "df_test = pd.read_csv(\"aiops_apm_testdata.csv\")\n",
    "#df_test = pd.read_csv(\"AvgResponseTime_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_test.METRIC_ATTRIBUTE.unique():\n",
    "    df_test[x]=(df_test.METRIC_ATTRIBUTE==x).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolation Forest\n",
    "#feature_columns = ['Average Response Time (ms)','MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']\n",
    "df_test['Average Response Time (ms)'] = df_test['VALUE_COUNT'].copy()\n",
    "data_test = df_test[feature_columns]\n",
    "#output_columns = ['Average Response Time (ms)','MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT','Result']\n",
    "scaler_test = StandardScaler()\n",
    "np_scaled_test = scaler_test.fit_transform(data_test)\n",
    "data_test = pd.DataFrame(np_scaled_test)\n",
    "# # train isolation forest\n",
    "results = pd.Series(saved_model.predict(data_test))\n",
    "#df_test['results'] = results\n",
    "df_test['anomaly2'] = results\n",
    "outdf = df_test.merge(df_test['anomaly2'].to_frame(), left_index=True, right_index=True)\n",
    "print(outdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new, 'normal' observations ----\n",
    "print(\"Accuracy:\", list(results).count(1)/results.shape[0])\n",
    "# Accuracy: 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict.py \n",
    "# Test Custom Predict File...\n",
    "# you may import any libraries you need here\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.multioutput import MultiOutputClassifier \n",
    "from cmlp.predictor import CMLPResponse, CMLPBasePredictor \n",
    "from sklearn.externals import joblib \n",
    "from io import BytesIO \n",
    "from io import BytesIO, StringIO \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from itertools import zip_longest \n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# This file must implement a Predictor class that extends the CMLPBasePredictor\n",
    "class Predictor(CMLPBasePredictor):\n",
    "\n",
    "    def predict(self, request):\n",
    "        # your predict function must return a CMLPResponse object\n",
    "        # you can return csv\n",
    "        df_in = pd.read_csv(StringIO(request.data.decode()))\n",
    "        categorical_columns=['Average Response Time (ms)']\n",
    "        continuous_columns=['MIN_VALUE', 'MAX_VALUE','AGG_VALUE','VALUE_COUNT']\n",
    "        feature_columns = categorical_columns + continuous_columns\n",
    "        output_columns=['Result']\n",
    "        for x in df_in.METRIC_ATTRIBUTE.unique():\n",
    "            df_in[x]=(df_in.METRIC_ATTRIBUTE==x).astype(int)\n",
    "            #print(df_in)\n",
    "        #print(df_test)\n",
    "        df_in = df_in.reset_index()\n",
    "        df_in['Average Response Time (ms)'] = df_in['VALUE_COUNT'].copy()\n",
    "        df_test = df_in[feature_columns]\n",
    "        scaler = StandardScaler()\n",
    "        np_scaled = scaler.fit_transform(df_test)\n",
    "        df_test = pd.DataFrame(np_scaled)\n",
    "        # # train isolation forest\n",
    "        results = pd.Series(self.model.predict(df_test))\n",
    "        #df_test['results'] = results\n",
    "        df_test['anomaly2'] = results\n",
    "        accuracy = list(results).count(1)/results.shape[0]\n",
    "        print(accuracy)\n",
    "        #df_test['accuracy'] = df_in.merge(df_test['accuracy'].to_frame(), left_index=True, right_index=True)\n",
    "        #outdf = df_test.merge(df_test['anomaly2'].to_frame(), left_index=True, right_index=True)\n",
    "        outdf = df_in.merge(df_test['anomaly2'].to_frame(), left_index=True, right_index=True)\n",
    "        return CMLPResponse(outdf.to_csv(index=True), content_type='text/csv')\n",
    "        #return CMLPResponse(outdf.to_csv(index=True), content_type='text/csv')\n",
    "    def load(self, model):\n",
    "        # you can load your model here. you may get the raw binary contents using 'model.contents'\n",
    "        # this method will be called before predict() is called, and you must save this model as load is only called\n",
    "        # once for a Predictor's lifecycle\n",
    "        # example of loading an sklearn model using joblib:\n",
    "        self.model = joblib.load(BytesIO(model.contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmlp.predictor import CMLPModel, CMLPRequest\n",
    "\n",
    "with open('aiops_apm_testdata.csv', 'rb') as input:\n",
    "    cmlp_request = CMLPRequest(input.read(), 'text/csv')\n",
    "predictor = Predictor()\n",
    "with open('aiops_apm.pkl', 'rb') as model:\n",
    "    predictor.load(CMLPModel(model.read()))\n",
    "cmlp_response = predictor.predict(cmlp_request)\n",
    "print(cmlp_response.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
